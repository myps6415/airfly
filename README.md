# airfly: Auto generate Airflow's `dag.py` on the fly

Data pipeline management is essential for data engineering in company, many data teams rely on tools like Airflow to help them organize workflows, such as ETL, data analytic jobs or machine learning projects.

Airflow provides rich extensibility to let developers arrange workloads into a sequence of "operators", and then declare the task dependencies within a `DAG` context while writing the `dag.py` file. 

As workflow grows progressively, the increasing complexity of task dependencies and code flexibility prones to messing up the dag structure, and decreases the maintainability, especially in collaborative scenarios.

`airfly` tries to mitigate such pain-points and brings automation to the development life cycle, it assumes all tasks are managed in certain python module, developers specify the dependencies while defining the task objects. During the deployment process, `airfly` resolves the dependency tree and automatically build the `dag.py` file for you.


## Concept

`airfly` expects the implementations are populated in a Python module(or package), the dependency can be declared by assigning `upstreams` and `downstreams` attributes to each task. The task objects are actually wrappers for Airflow operators, when `airfly` walks recursively through the entire module, all tasks are discovered and collected, the dependency tree and the `DAG` context are automatically built. Finally, the code generation is completed with some `ast` helpers.

<img src="assets/layout.png" width="500">


## Usage

### Wrap Airflow operator with `AirflowTask`
In order to generate codes for Airflow operators, we implement `AirflowTask` to hold certain information.

```python
# in tutorial.py
from airfly.model.airflow import AirflowTask


class print_date(AirflowTask):
    operator_class = "BashOperator" 
    params = dict(bash_command="date")

```

* `operator_class` specifies the class of the Airflow operator.
* The class name(`print_date`) will be mapped to `task_id` to the applied operator after code generation.
* `params` will be passed to operator as keyword argument.


### Declare task dependency

Use `upstreams` or `downstreams` to specify task dependencies.

```python
# in tutorial.py

from textwrap import dedent


templated_command = dedent(
    """
{% for i in range(5) %}
    echo "{{ ds }}"
    echo "{{ macros.ds_add(ds, 7)}}"
    echo "{{ params.my_param }}"
{% endfor %}
"""
)

class templated(AirflowTask):
    operator_class = "BashOperator"
    params = dict(depends_on_past=False,
                  bash_command=templated_command,
                  params={"my_param": "Parameter I passed in"})


class sleep(AirflowTask):
    operator_class = "BashOperator"
    params = dict(depends_on_past=False, 
                  bash_command="sleep 5",
                  retries=3)

    upstreams = print_date
    downstreams = [templated]

```

### Generate the `dag.py` file

```
$ airfly --name tutorial_dag --modname tutorial > dag.py
```

generated codes in `dag.py`:

```python
# This file is auto-generated by airfly 0.4.0
from airflow.models import DAG
from airflow.operators.bash import BashOperator

with DAG("tutorial_dag") as dag:
    tutorial_print_date = BashOperator(
        task_id="tutorial.print_date", bash_command="date"
    )
    tutorial_sleep = BashOperator(
        task_id="tutorial.sleep",
        depends_on_past=False,
        bash_command="sleep 5",
        retries=3,
    )
    tutorial_templated = BashOperator(
        task_id="tutorial.templated",
        depends_on_past=False,
        bash_command="""
{% for i in range(5) %}
    echo "{{ ds }}"
    echo "{{ macros.ds_add(ds, 7)}}"
    echo "{{ params.my_param }}"
{% endfor %}
""",
        params={"my_param": "Parameter I passed in"},
    )
    tutorial_print_date >> tutorial_sleep
    tutorial_sleep >> tutorial_templated

```


## Assign `DAG` parameters


## Exclude tasks from dependency tree
